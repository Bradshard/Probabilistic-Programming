# -*- coding: utf-8 -*-
"""hw4_COGS516.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pmJ5PVm7vpPiqTdb1zOE92RChgf0lVBm

# COGS516 - Assignment 4

Please enter your **name, surname** and **student number** instead of `"NAME-HERE"`, `"SURNAME-HERE"`, `"NUMBER-HERE"` below
"""

student = {
    'name' : "Abdullah-Burkan" ,
    'surname' : "Bereketoğlu", 
    'studentNumber' : "2355170"
}

print(student)

import numpy as np
import pandas as pd
import pymc3 as pm
import matplotlib.pyplot as plt
import arviz as az
import daft

"""The `Recall.csv` dataset is from Oberauer (2019) which investigate the capacity of working memory. In this study, each subject was presented word lists with different lengths (containing 2, 4, 6 and 8 words). Afterwards, they were asked to recall a word given its position in the list. Figure shows an example of set size 4. 

<!-- ![](./fig_memory.png) -->
![fig_memory.png](attachment:fig_memory.png)

The dataset contains the following variables:

- `set_size`: the number of words shown to the subject
- `correct`: 1 if the answer was correct, 0 otherwise
- `tested`: the position of the word asked
- The other variables in the dataset indicates the subject, trial, session and block ids.


> Oberauer, Klaus. 2019. “Working Memory Capacity Limits Memory for Bindings.” Journal of Cognition 2 (1): 40. https://doi.org/10.5334/joc.86
"""

d = pd.read_csv("Recall.csv")

d.head()

"""It is known that as the number of items to be held in working memory increase (`set_size` or $S$), the accuracy decraease (`correct` or $C$). The number of items shown ($S$) also affects where the test item will be (`tested` or $T$). Finally, the location of the test item ($T$) can also affect accuracy ($C$) as items that are shown later can be easier to remember. 

1. Draw a DAG for this problem using the `daft` package.
"""

import networkx as nx

graph = nx.DiGraph()
graph.add_edges_from([("S","T"),("T","C"),("S","C")])

nx.draw(graph, with_labels=True)

"""Daft was not working in google colab and Ipython was having problems with Int32 and INT64 so I used colab instead of Jupyter.

1. Estimate the **total effect** of `set_size` on `correct`. Note that, the outcome variable is a categorical 0/1 variable. You should follow the Bayesian workflow when approching this problem, which includes:
- Scale your inputs when necessary.
- Do prior predictive simulation to choose suitable priors
- Compute the posteriors
- Ensure that your MCMC samples from the posterior using suitable graphical tools and metrics for assessment.
- Describe how the posterior parameters should be interpreted(making suitable transformations when necessary)
- Run posterior predictive simulation and describe the results
"""

# your code here
d_1 = d[['set_size', 'correct', 'tested']].copy()
d_1.head()

# for total effect no adjustments necessary

i1 = 0
i2 = 0
i3 = 0
i4 = 0

for i in d_1["set_size"]:
    if i == 2:
        i1 += 1
    if i == 4:
        i2 += 1
    if i == 6:
        i3 += 1
    if i == 8:
        i4 += 1

i1

i2

i3

i4

d['set_size'] = d['set_size'].map({2:'2', 4:"4", 6:'6', 8:'8'}) 
d['tested2'] = d['tested'].map({1:'1',2:'2',3:'3',4:'4',5:'5',6:'6',7:'7',8:'8'})

d["gid"] = pd.Categorical(d["tested2"]).codes 
d["did"] = pd.Categorical(d["set_size"]).codes
d.dtypes

with pm.Model() as tot_Eff:
    a = pm.Normal('a', 0,1, shape = 4)
    p = pm.math.invlogit(a[d.did])
    A = pm.Binomial('A', p = p, n = d.tested, observed = d.correct)
    trace_tot_eff = pm.sample(1000,chains=4, return_inferencedata=True)

trace_tot_eff

az.summary(trace_tot_eff)

"""Means show us that with increasing set size we have lower chance of getting correct answers and that was what we wanted to get, again with ess and r_hat values we can prove that means are true and with increasing set size getting correct is decreasing."""

az.summary(trace_tot_eff, kind="diagnostics")

"""Diagnostic results for mcse's indicate the results are reliable. Later on Ess results for both bulk and tail shows us that effective number of samples are high which indicates samples are reliable, Rhat shows no divergence between- and within- chains again indication of reliable chain and posterior sampling."""

az.plot_trace(trace_tot_eff, kind="rank_bars",compact = False)

"""Rank Bars and Plots of Trace plots show us small or slight discrepancies on all chains so we can say that chains that are produced indeed are good ones and can be relied on.

They also indicate that our priors are not bad but good ones for the Total Effect Model.
"""

correct_pred = pm.sample_posterior_predictive(trace_tot_eff, 200, tot_Eff)

az.plot_ppc(az.from_pymc3(posterior_predictive=correct_pred, model=tot_Eff))

"""Here one can see that from the posterior predictive mean plot of orange dashed lines for our Set_Size being 2 we get more correct and the correct percentage decreases with the Set_Size increases.

- What is the difference of probability of correct answer (contrast) between showing 6 and 2 items to the participants? (Simulate interventions for these two conditions with your posteriors, and show the contrast distribution using a suitable graph)

Here firstly we will set the set_size to 2 later on we will make them 6 and compare the effects on a plot.
"""

# your code here
from theano import shared
set_sizes = d_1.set_size.values
set_sizes_shared = shared(set_sizes)
set_2 = np.full(len(set_sizes),2)
set_sizes_shared.set_value(set_2)

with tot_Eff:
    tot_Eff_2 = pm.sample_posterior_predictive(trace_tot_eff)

"""We did the set_size 2 now we will do it for set_size 6

"""

set_sizes_2 = d_1.set_size.values
set_sizes_shared_2 = shared(set_sizes_2)
set_6 = np.full(len(set_sizes_2),6)
set_sizes_shared_2.set_value(set_6)

with tot_Eff:
    tot_Eff_6 = pm.sample_posterior_predictive(trace_tot_eff)

"""Here we will do the the difference plot."""

ax = az.plot_dist(tot_Eff_2["A"] - tot_Eff_6["A"], color="k")
ax.set_xlabel("posterior predicted difference contrast of correct (DBAC)")
ax.set_ylabel("density")

"""I wanted to do a different plot, but only could came up with this one.

2. Estimate the **direct effect** of `set_size` on `correct` by following the Bayesian workflow. Describe how the posterior parameters should be interpreted(making suitable transformations when necessary)

Here we made our categoric model by using both tested and set_size as categories (which may be wrong but I assumed they looked like categories.)

And added them to logit function and made a binomial distribution for the correct values which also indicates categoric values.
"""

# your code here
with pm.Model() as tot_direct_Eff: 
    a = pm.Normal('a', 0, 1, shape = [4,8]) 
    p = pm.math.invlogit(a[d.did,d.gid]) 
    A = pm.Binomial('A', p = p, n = d_1.tested, observed = d_1.correct )  
    trace_dir_eff = pm.sample(1000, return_inferencedata=True)

# for direct effect T should be adjusted

az.summary(trace_dir_eff)

"""It is important to note that some of these a values does not make sense since for a set_size of 6 cannot ask a test index of 7 or 8, so some of the samples that are taken from the system are not used and neglected.

Ess values gives us great effective sample size scores for each value both for tail and bulk so we can say these values are reliable
"""

az.summary(trace_dir_eff, kind="diagnostics")

"""R_hat values and mmse values for all of the different test values and set_sizes are given. 

R_hat values are 1 which is great for all, that means values are converged and it also shows the convergence diagnostics that compares between and within chain estimates. So 1 as the score of Rhat will give us chains that are mixed well and between-within chain estimates agree. But if they were high that would not be the case.

Same also applies to MMSE scores.
"""

az.plot_trace(trace_dir_eff, kind="rank_bars",compact = False)

"""Rank Bars and Plots of Trace plots show us small or slight discrepancies on all chains so we can say that chains that are produced indeed are good ones and can be relied on. 

They also indicate that our priors are not bad but good ones for the Direct Effect Model.

3. Compare the models you used for estimating the *total effect* and *direct effect* by using WAIC as your comparison measure. Discuss the predictive performance of these two models based on this.
"""

# your code here

compare_dict = {"Direct":trace_dir_eff, "Total":trace_tot_eff}

az.compare(compare_dict, ic ="waic")

"""# Comparison Analysis for WAIC


As from the comparison of the Direct effect model of the Set_size to Correct answer and also the Set_size to Tested to Correct answer which gives us the Total effect, indicates that Direct Effect model is a greater model. 

Since both models give warning False, we shouldn't worry about their results are misinterpreted, they are not. 

For negative-log scale which is what WAIC uses it indicates that smaller values of WAIC shows higher out-of-sample predictive fit or better model, so for that we can say that Direct Estimate is again better in absolute smaller value. 

p_waic => shows us the estimated effective number of parameters and Direct Effect model indicates higher n_eff so we can say that Direct is also better.

Weight shows us the relative weights of the models and it indicates that probability of the models and direct has a higher score on that. 

D_waic ==> shows us relative differences between the Direct and Total so since Direct is ranked the best, but their difference with total is 11.5 so it indicates they are distanced from each other as models for each estimate.

SE is the standard error and lower standard error is better but for the best one (Direct) for this question is higher.

Warning as mentioned above indicates computations of the information criterion can be mislead or unreliable, but for our testing both are reliable.

As the result ==> Direct Effect of Set_Size to Correct model is better for prediction then Total Effect

You may add more code or markdown blocks above between the questions as you see necessary.
"""